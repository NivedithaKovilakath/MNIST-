# -*- coding: utf-8 -*-
"""mnist_NN_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dzlWOj1uJdRVKSSW1xRDoVcQ9T_4Pv_5
"""

import numpy as np
import tensorflow as tf
from keras.utils.np_utils import to_categorical
import pandas

#importing MNIST dataset

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Forming arrays for the input and output data

x_train=np.array(x_train)
x_test=np.array(x_test)

y_train=np.array(y_train)
y_test=np.array(y_test)

#to flatten the arrays

l=len(x_train)

x_train=x_train.reshape(l,-1)


m=len(x_test)
x_test=x_test.reshape(m,-1)

#slice

x_train=x_train[:1000]
x_test=x_test[:1000]
y_train=y_train[:1000]
y_test=y_test[:1000]

x_train = (x_train/255).astype('float32')
x_test = (x_test/255).astype('float32')
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

import numpy as np
a= np.random.rand(8,13)
print(a)

class neural_network():
  def __init__(self, sizes, epochs=10, l_rate=0.001):
    self.sizes=sizes
    self.epochs=epochs
    self.l_rate=l_rate
    #save the parameters in this dictionary
    self.params=self.initialization() 

  #no of nodes in each layer 
  def initialization(self):
    input=self.sizes[0]
    hidden_1=self.sizes[1]
    hidden_2=self.sizes[2]
    output=self.sizes[3]
    params={
        'W1':np.random.randn(hidden_1, input) * np.sqrt(1. / hidden_1),
        'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),
        'W3':np.random.randn(output, hidden_2) * np.sqrt(1. / output)
    }

    
    return params

  def forward_pass(self, x_train):
    params=self.params
    #input layer
    params['A0']=x_train

    #input to hidden layer 1
    params['Z1']=np.dot(params['W1'], params['A0'].T)
    params['A1']=self.sigmoid(params['Z1'])

    #hidden layer 1 to 2
    params['Z2']=np.dot(params['W2'], params['A1'])
    params['A2']=self.sigmoid(params['Z2'])

    #hidden layer 2 to output
    params['Z3']=np.dot(params['W3'], params['A2'])
    params['A3']=self.softmax(params['Z3'])
    
    return (params['A3'])

  def sigmoid(self, x, derivative=False):
    if derivative:
      return(np.exp(-x))/(np.exp(-x)+1)**2
    return 1/(1+np.exp(-x))

  def softmax(self, x):
    exps=np.exp(x-x.max())
    return exps/np.sum(exps, axis=0)

  def backward_pass(self, y_train, output):
    params=self.params
    change_w={}
    #calcuate w3 update
    error = 2 * (output - y_train.T)/output.shape[0] * self.softmax(params['Z3'])
    change_w['W3']=np.dot(error,params['A2'].T)
    

    #calculate w2 update
    error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)

    change_w['W2'] = np.dot(error, params['A1'].T)

    #calculate w1 error
    error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True) 
    
    change_w['W1'] = np.dot(error, params['A0'])

    return change_w

  def train(self, x_train, y_train, x_test, y_test):
    for i in range(self.epochs):
      for x, y in zip(x_train, y_train):
        output=self.forward_pass(x_train)
        change_to_w= self.backward_pass(y_train, output)
        self.update_network_parameters(change_to_w)

      accuracy=self.compute_accuracy(x_test, y_test)
      print('Epoch: {0}, accuracy: {1}'.format(i+1, accuracy))

  

  def update_network_parameters(self, change_to_w):
    for key, value in change_to_w.items():
       self.params[key]-= self.l_rate * value
  

  def compute_accuracy(self, x_test, y_test):
    predictions=[]

    for x, y in zip(x_test, y_test):
      output=self.forward_pass(x)
      pred=np.argmax(output)
      predictions.append(pred==np.argmax(y))

    return np.mean(predictions)



nn=neural_network([784,128,64,10])
nn.train(x_train, y_train, x_test, y_test)